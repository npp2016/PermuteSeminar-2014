Week 12 Summary
========================================================
Note below adapted from Sam, Emily R and Rome
Note that the Anderson et al. paper is kind of a short version of their book on the same subject


Thre traditional approach to model selection now is the Akaike Information Criterion (AIC) but the Anderson et al. paper suggests it my not be the best way to select a model

The idea of models is that you are trying to fit complex "real world" mechanisms to simplistic explanations. The less parameters you have, the more simplistic your model is. But whaen your model gets too complex, you could end up just fitting noise, which is meaningless. Additionally, when adding parameters, there is a tradeoff between bias and variance. This is why AIC weighs the likelihoods of models with penalty costs to the number of parameters they have.

AIc holds an advantage over stepwise regression because AIC can compare any model using the same data, not just nested ones.

Background and Math - Thanks to Emily R for these notes
------------------------------------------------------
  
  As described in the Anderson et al. paper, traditional methods of model selection have a lot of problems.  They advocate an alternative approach using Akaike's Information Criterion,

\[
AIC = -2 \log \mathcal L + 2 k 
\]

where $\mathcal L$ is the model's likelihood and $k$ is the number of fitted parameters in the model.  The absolute value of $AIC$ is not informative, but within a set of models--*provided they are fit to the same data*--the model with the *lowest* $AIC$ is theoretically the best, in terms of optimizing the bias-variance tradeoff.

The corrected version for small sample sizes ($AICc$) adds another term on the end:
  \[
    AICc = -2 \log \mathcal L + 2 k + \frac{2k(k+1)}{n - k - 1}
    \]
Here, $n$ is the number of data points.  The original $AIC$ tends to be too liberal when sample sizes are small, encouraging overfitting.  The extra penalty term in $AIC_c$ corrects for this.  As $n$ gets much larger than the number of parameters, the two versions converge, so there's no reason not to use $AICc$ all the time.  There a bunch of criteria like this, similar to or derived from $AIC$, and all work more or less the same way.  For generality, I'll use "AIC" as a catch-all term here.

Again, the absolute value of $AIC$ is meaningless--it is the relative values within a set of candidate models that is important.  As such, we are usually more interested in the difference between a given model's $AIC$ value and the lowest value in the set:

\[
\Delta_i = AIC_i - AIC_{min}
\]

This allows us to rank the models from best to worst.  As a rule of thumb, models with delta-AIC values less than 2 or 3 units apart have a pretty similar level of support.  If several models are close to the best one, we can't really be confident that the model ranking would remain the same if we were able to collect a new data set and do the model-selection procedure again.  

To get at this uncertaintly more rigorously, we can calculate the "Akaike weight" for each model:
  \[
    w_i = \frac{\exp \left(-\Delta_i/2 \right)}{\sum_{r=1}^{R} \exp \left( -\Delta_r / 2 \right)}
    \]
The weights add up to 1, and represent the probability that each model would be chosen as the best if the model selection procedure were run again on a new dataset generated by the same underlying process.

It is also possible to obtain similar results by boostrapping a dataset and fitting the models to the simulated data, measuring how many times a model is identified as the best on boostrapped data, empirically calculating the probability of a model to be selected.

Data and coding
---------------
  
  All of the AIC-based procedures are conveniently available in the `AICcmodavg` package.

```r
require(AICcmodavg)
```

```
## Loading required package: AICcmodavg
```

```r
require(RCurl)
```

```
## Loading required package: RCurl
## Loading required package: bitops
```



We will be using some data from a simulated survey of an imaginary mountain valley. 


<img src="/Figures/landscape.png">
  
  You have spent an imaginary summer hiking all over the place, sampling random locations.  At each location, you have recorded whether the bird of interest was present, along with the percent cover of each of three plant species.  The high elevations are mostly bare rock and the valley floor is mostly farmland; there are variables coding for these types of cover as well.



```r
url.bhab <- getURL("https://raw.githubusercontent.com/PermuteSeminar/PermuteSeminar-2014/master/Week-12/bird_habitat.csv")

bhab <- read.csv(text = url.bhab)

getAICtab <- function(bhab) {
    mod1 <- glm(bird.present ~ elevation, bhab, family = binomial)
    mod2 <- glm(bird.present ~ farm, bhab, family = binomial)
    mod3 <- glm(bird.present ~ northing + easting, bhab, family = binomial)
    tab <- aictab(list(mod1, mod2, mod3), modnames = c("Elevation", "Farm", 
        "Position"))
    return(as.character(tab[1, 1]))
}

vec <- c()
for (i in 1:500) {
    bootROW <- sample(1:nrow(bhab), nrow(bhab), replace = T)
    vec[i] <- getAICtab(bhab[bootROW, ])
}

table(vec)
```

```
## vec
## Elevation      Farm  Position 
##         8       370       122
```

The code was Jon's solution, The 'vec' table returns the number of times each model was the best supported out of 500 boostraps, allowing to select the best model.
